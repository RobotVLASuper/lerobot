{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f42f852-922a-45af-bf82-fbc156f8de76",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "454ed22e-0629-4608-a826-67b4760ea4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import imageio\n",
    "import torch\n",
    "\n",
    "import lerobot\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d73c1af-cfaf-4a9d-a6bf-33c6e45669fd",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b49acf4-ad4a-4170-b129-a7e2e8f48958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "# print(\"List of available datasets:\")\n",
    "# pprint(lerobot.available_datasets)\n",
    "\n",
    "repo_id = \"lerobot/aloha_sim_insertion_human\"\n",
    "# You can easily load a dataset from a Hugging Face repository\n",
    "dataset = LeRobotDataset(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f422e316-ff93-4137-b8f9-090cfac3cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeRobotDataset(\n",
      "  Repository ID: 'lerobot/aloha_sim_insertion_human',\n",
      "  Split: 'train',\n",
      "  Number of Samples: 25000,\n",
      "  Number of Episodes: 50,\n",
      "  Type: video (.mp4),\n",
      "  Recorded Frames per Second: 50,\n",
      "  Camera Keys: ['observation.images.top'],\n",
      "  Video Frame Keys: ['observation.images.top'],\n",
      "  Transformations: None,\n",
      "  Codebase Version: v1.6,\n",
      ")\n",
      "Dataset({\n",
      "    features: ['observation.images.top', 'observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.done', 'index'],\n",
      "    num_rows: 25000\n",
      "})\n",
      "\n",
      "average number of frames per episode: 500.000\n",
      "frames per second used during data collection: dataset.fps=50\n",
      "keys to access images from cameras: dataset.camera_keys=['observation.images.top']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LeRobotDataset is actually a thin wrapper around an underlying Hugging Face dataset\n",
    "# (see https://huggingface.co/docs/datasets/index for more information).\n",
    "print(dataset)\n",
    "print(dataset.hf_dataset)\n",
    "\n",
    "# And provides additional utilities for robotics and compatibility with Pytorch\n",
    "print(f\"\\naverage number of frames per episode: {dataset.num_samples / dataset.num_episodes:.3f}\")\n",
    "print(f\"frames per second used during data collection: {dataset.fps=}\")\n",
    "print(f\"keys to access images from cameras: {dataset.camera_keys=}\\n\")\n",
    "\n",
    "# Access frame indexes associated to first episode\n",
    "episode_index = 0\n",
    "from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "to_idx = dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "\n",
    "# LeRobot datasets actually subclass PyTorch datasets so you can do everything you know and love from working\n",
    "# with the latter, like iterating through the dataset. Here we grab all the image frames.\n",
    "frames = [dataset[idx][\"observation.images.top\"] for idx in range(from_idx, to_idx)]\n",
    "\n",
    "# Video frames are now float32 in range [0,1] channel first (c,h,w) to follow pytorch convention. To visualize\n",
    "# them, we convert to uint8 in range [0,255]\n",
    "frames = [(frame * 255).type(torch.uint8) for frame in frames]\n",
    "# and to channel last (h,w,c).\n",
    "frames = [frame.permute((1, 2, 0)).numpy() for frame in frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "155e0586-47ea-4e13-8bb3-36b0ac54c50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"outputs/examples/1_load_lerobot_dataset/episode_0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "# Finally, we save the frames to a mp4 video for visualization.\n",
    "dir_path = \"outputs/examples/1_load_lerobot_dataset\"\n",
    "Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "video_path = dir_path + \"/episode_0.mp4\"\n",
    "imageio.mimsave(video_path, frames, fps=dataset.fps)\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d75427d-1fc7-408b-937a-a1c998878537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 56 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56/56 [00:00<00:00, 1539.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataset[0]['observation.images.top'].shape=torch.Size([4, 3, 480, 640])\n",
      "dataset[0]['observation.state'].shape=torch.Size([8, 14])\n",
      "dataset[0]['action'].shape=torch.Size([64, 14])\n",
      "\n",
      "batch['observation.images.top'].shape=torch.Size([32, 4, 3, 480, 640])\n",
      "batch['observation.state'].shape=torch.Size([32, 8, 14])\n",
      "batch['action'].shape=torch.Size([32, 64, 14])\n"
     ]
    }
   ],
   "source": [
    "# For many machine learning applications we need to load the history of past observations or trajectories of\n",
    "# future actions. Our datasets can load previous and future frames for each key/modality, using timestamps\n",
    "# differences with the current loaded frame. For instance:\n",
    "delta_timestamps = {\n",
    "    # loads 4 images: 1 second before current frame, 500 ms before, 200 ms before, and current frame\n",
    "    \"observation.images.top\": [-1, -0.5, -0.20, 0],\n",
    "    # loads 8 state vectors: 1.5 seconds before, 1 second before, ... 20 ms, 10 ms, and current frame\n",
    "    \"observation.state\": [-1.5, -1, -0.5, -0.20, -0.10, -0.02, -0.01, 0],\n",
    "    # loads 64 action vectors: current frame, 1 frame in the future, 2 frames, ... 63 frames in the future\n",
    "    \"action\": [t / dataset.fps for t in range(64)],\n",
    "}\n",
    "dataset = LeRobotDataset(repo_id, delta_timestamps=delta_timestamps)\n",
    "print(f\"\\n{dataset[0]['observation.images.top'].shape=}\")  # (4,c,h,w)\n",
    "print(f\"{dataset[0]['observation.state'].shape=}\")  # (8,c)\n",
    "print(f\"{dataset[0]['action'].shape=}\\n\")  # (64,c)\n",
    "\n",
    "# Finally, our datasets are fully compatible with PyTorch dataloaders and samplers because they are just\n",
    "# PyTorch datasets.\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=0,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "for batch in dataloader:\n",
    "    print(f\"{batch['observation.images.top'].shape=}\")  # (32,4,c,h,w)\n",
    "    print(f\"{batch['observation.state'].shape=}\")  # (32,8,c)\n",
    "    print(f\"{batch['action'].shape=}\")  # (32,64,c)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "699cebdb-9edb-45a0-89c8-06085e09f7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0281, -0.5264,  0.8721, -0.0157,  0.4200, -0.0215,  0.1536, -0.0547,\n",
       "        -0.8091,  0.9081,  0.0494,  0.3163,  0.1224, -0.0032])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['observation.state'][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7236e1-53ea-4f6f-96cf-e0f344282c6e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f4eda23-6f01-4e2a-bf97-3a87d8c2f761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 25650 examples [00:00, 381978.62 examples/s]\n",
      "Fetching 212 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 212/212 [00:07<00:00, 28.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 loss: 1.193\n",
      "step: 250 loss: 0.067\n",
      "step: 500 loss: 0.054\n",
      "step: 750 loss: 0.056\n",
      "step: 1000 loss: 0.049\n",
      "step: 1250 loss: 0.057\n",
      "step: 1500 loss: 0.044\n",
      "step: 1750 loss: 0.048\n",
      "step: 2000 loss: 0.050\n",
      "step: 2250 loss: 0.068\n",
      "step: 2500 loss: 0.045\n",
      "step: 2750 loss: 0.025\n",
      "step: 3000 loss: 0.045\n",
      "step: 3250 loss: 0.025\n",
      "step: 3500 loss: 0.040\n",
      "step: 3750 loss: 0.041\n",
      "step: 4000 loss: 0.052\n",
      "step: 4250 loss: 0.034\n",
      "step: 4500 loss: 0.028\n",
      "step: 4750 loss: 0.040\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.common.policies.diffusion.configuration_diffusion import DiffusionConfig\n",
    "from lerobot.common.policies.diffusion.modeling_diffusion import DiffusionPolicy\n",
    "\n",
    "# Create a directory to store the training checkpoint.\n",
    "output_directory = Path(\"outputs/train/example_pusht_diffusion\")\n",
    "output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Number of offline training steps (we'll only do offline training for this example.)\n",
    "# Adjust as you prefer. 5000 steps are needed to get something worth evaluating.\n",
    "training_steps = 5000\n",
    "device = torch.device(\"cuda\")\n",
    "log_freq = 250\n",
    "\n",
    "# Set up the dataset.\n",
    "delta_timestamps = {\n",
    "    # Load the previous image and state at -0.1 seconds before current frame,\n",
    "    # then load current image and state corresponding to 0.0 second.\n",
    "    \"observation.image\": [-0.1, 0.0],\n",
    "    \"observation.state\": [-0.1, 0.0],\n",
    "    # Load the previous action (-0.1), the next action to be executed (0.0),\n",
    "    # and 14 future actions with a 0.1 seconds spacing. All these actions will be\n",
    "    # used to supervise the policy.\n",
    "    \"action\": [-0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4],\n",
    "}\n",
    "dataset = LeRobotDataset(\"lerobot/pusht\", delta_timestamps=delta_timestamps)\n",
    "\n",
    "# Set up the the policy.\n",
    "# Policies are initialized with a configuration class, in this case `DiffusionConfig`.\n",
    "# For this example, no arguments need to be passed because the defaults are set up for PushT.\n",
    "# If you're doing something different, you will likely need to change at least some of the defaults.\n",
    "cfg = DiffusionConfig()\n",
    "policy = DiffusionPolicy(cfg, dataset_stats=dataset.stats)\n",
    "policy.train()\n",
    "policy.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)\n",
    "\n",
    "# Create dataloader for offline training.\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=4,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=device != torch.device(\"cpu\"),\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# Run training loop.\n",
    "step = 0\n",
    "done = False\n",
    "while not done:\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "        output_dict = policy.forward(batch)\n",
    "        loss = output_dict[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % log_freq == 0:\n",
    "            print(f\"step: {step} loss: {loss.item():.3f}\")\n",
    "        step += 1\n",
    "        if step >= training_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "# Save a policy checkpoint.\n",
    "policy.save_pretrained(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee042e-43de-489c-9ce5-497a04a48653",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "771c90ad-a094-42ad-ba81-271fa27c1951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mshukor/envs/lerobot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n",
      "GPU is available. Device set to: cuda\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import gym_pusht  # noqa: F401\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import numpy\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from lerobot.common.policies.diffusion.modeling_diffusion import DiffusionPolicy\n",
    "from lerobot.common.policies.act.modeling_act import ACTPolicy\n",
    "\n",
    "\n",
    "\n",
    "# Download the diffusion policy for pusht environment\n",
    "# pretrained_policy_path = Path(snapshot_download(\"lerobot/diffusion_pusht\"))\n",
    "# OR uncomment the following to evaluate a policy from the local outputs/train folder.\n",
    "\n",
    "# Create a directory to store the video of the evaluation\n",
    "# output_directory = Path(\"outputs/eval/example_pusht_diffusion\")\n",
    "# output_directory.mkdir(parents=True, exist_ok=True)\n",
    "# pretrained_policy_path = Path(\"outputs/train/example_pusht_diffusion\")\n",
    "# policy = DiffusionPolicy.from_pretrained(pretrained_policy_path)\n",
    "\n",
    "TASK_NAME = \"lerobot_base_distributed_aloha_transfer_cube_1gpus\"\n",
    "step = 50000\n",
    "# Create a directory to store the video of the evaluation\n",
    "output_directory = Path(f\"outputs/eval/{TASK_NAME}\")\n",
    "output_directory.mkdir(parents=True, exist_ok=True)\n",
    "policy_path = f\"/data/mshukor/logs/lerobot/{TASK_NAME}/checkpoints/{step:06d}/pretrained_model\"\n",
    "pretrained_policy_path = Path(policy_path)\n",
    "policy = ACTPolicy.from_pretrained(pretrained_policy_path)\n",
    "\n",
    "\n",
    "policy.eval()\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Device set to:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"GPU is not available. Device set to: {device}. Inference will be slower than on GPU.\")\n",
    "    # Decrease the number of reverse-diffusion steps (trades off a bit of quality for 10x speed)\n",
    "    policy.diffusion.num_inference_steps = 10\n",
    "\n",
    "policy.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c9151b7-2298-421c-b63b-d97371875730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_aloha \n",
    "import gym_xarm \n",
    "# Initialize evaluation environment to render two observation types:\n",
    "# an image of the scene and state/position of the agent. The environment\n",
    "# also automatically stops running after 300 interactions/steps.\n",
    "# env = gym.make(\n",
    "#     \"gym_pusht/PushT-v0\",\n",
    "#     obs_type=\"pixels_agent_pos\",\n",
    "#     max_episode_steps=300,\n",
    "# )\n",
    "\n",
    "# env = gym.make(\n",
    "#     \"gym_aloha/AlohaTransferCube-v0\",\n",
    "#     obs_type=\"pixels_agent_pos\",\n",
    "#     max_episode_steps=300,\n",
    "# )\n",
    "\n",
    "env = gym.make(\n",
    "    \"gym_xarm/XarmLift-v0\",\n",
    "    obs_type=\"pixels_agent_pos\",\n",
    "    max_episode_steps=300,\n",
    ")\n",
    "\n",
    "# Reset the policy and environmens to prepare for rollout\n",
    "policy.reset()\n",
    "numpy_observation, info = env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdebf0bd-46d5-4d53-ad56-61edabd41ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0 reward=0.0 terminated=False\n",
      "step=1 reward=0.0 terminated=False\n",
      "step=2 reward=0.0 terminated=False\n",
      "step=3 reward=0.0 terminated=False\n",
      "step=4 reward=0.0 terminated=False\n",
      "step=5 reward=0.0 terminated=False\n",
      "step=6 reward=0.0 terminated=False\n",
      "step=7 reward=0.0 terminated=False\n",
      "step=8 reward=0.0 terminated=False\n",
      "step=9 reward=0.0 terminated=False\n",
      "step=10 reward=0.0 terminated=False\n",
      "step=11 reward=0.0 terminated=False\n",
      "step=12 reward=0.0 terminated=False\n",
      "step=13 reward=0.0 terminated=False\n",
      "step=14 reward=0.0 terminated=False\n",
      "step=15 reward=0.0 terminated=False\n",
      "step=16 reward=0.0 terminated=False\n",
      "step=17 reward=0.0 terminated=False\n",
      "step=18 reward=0.0 terminated=False\n",
      "step=19 reward=0.0 terminated=False\n",
      "step=20 reward=0.0 terminated=False\n",
      "step=21 reward=0.0 terminated=False\n",
      "step=22 reward=0.0 terminated=False\n",
      "step=23 reward=0.0 terminated=False\n",
      "step=24 reward=0.0 terminated=False\n",
      "step=25 reward=0.0 terminated=False\n",
      "step=26 reward=0.0 terminated=False\n",
      "step=27 reward=0.0 terminated=False\n",
      "step=28 reward=0.0 terminated=False\n",
      "step=29 reward=0.0 terminated=False\n",
      "step=30 reward=0.0 terminated=False\n",
      "step=31 reward=0.0 terminated=False\n",
      "step=32 reward=0.0 terminated=False\n",
      "step=33 reward=0.0 terminated=False\n",
      "step=34 reward=0.0 terminated=False\n",
      "step=35 reward=0.0060973941747585235 terminated=False\n",
      "step=36 reward=0.02622279916580214 terminated=False\n",
      "step=37 reward=0.05121513810520591 terminated=False\n",
      "step=38 reward=0.07973351723518816 terminated=False\n",
      "step=39 reward=0.1055264807998946 terminated=False\n",
      "step=40 reward=0.11397255143592261 terminated=False\n",
      "step=41 reward=0.1183477905742287 terminated=False\n",
      "step=42 reward=0.12188789988102575 terminated=False\n",
      "step=43 reward=0.12545805998888346 terminated=False\n",
      "step=44 reward=0.1288567941022673 terminated=False\n",
      "step=45 reward=0.13151607159645246 terminated=False\n",
      "step=46 reward=0.1334215324045937 terminated=False\n",
      "step=47 reward=0.13462679837021455 terminated=False\n",
      "step=48 reward=0.13541090727032962 terminated=False\n",
      "step=49 reward=0.13590309886928334 terminated=False\n",
      "step=50 reward=0.13633914976509004 terminated=False\n",
      "step=51 reward=0.13675125461247126 terminated=False\n",
      "step=52 reward=0.13710415750492036 terminated=False\n",
      "step=53 reward=0.1373384382180125 terminated=False\n",
      "step=54 reward=0.13744056010263833 terminated=False\n",
      "step=55 reward=0.1373603108492143 terminated=False\n",
      "step=56 reward=0.13713720481440303 terminated=False\n",
      "step=57 reward=0.13711259085950156 terminated=False\n",
      "step=58 reward=0.13711259085950156 terminated=False\n",
      "step=59 reward=0.13711259085950156 terminated=False\n",
      "step=60 reward=0.13711259085950156 terminated=False\n",
      "step=61 reward=0.13711259085950156 terminated=False\n",
      "step=62 reward=0.13711259085950156 terminated=False\n",
      "step=63 reward=0.13711259085950156 terminated=False\n",
      "step=64 reward=0.13711259085950156 terminated=False\n",
      "step=65 reward=0.13711259085950156 terminated=False\n",
      "step=66 reward=0.13711259085950156 terminated=False\n",
      "step=67 reward=0.13711259085950156 terminated=False\n",
      "step=68 reward=0.13711259085950156 terminated=False\n",
      "step=69 reward=0.13711259085950156 terminated=False\n",
      "step=70 reward=0.13711259085950156 terminated=False\n",
      "step=71 reward=0.13711259085950156 terminated=False\n",
      "step=72 reward=0.13711259085950156 terminated=False\n",
      "step=73 reward=0.13711259085950156 terminated=False\n",
      "step=74 reward=0.13711259085950156 terminated=False\n",
      "step=75 reward=0.13711259085950156 terminated=False\n",
      "step=76 reward=0.13711259085950156 terminated=False\n",
      "step=77 reward=0.13711259085950156 terminated=False\n",
      "step=78 reward=0.13711259085950156 terminated=False\n",
      "step=79 reward=0.13711259085950156 terminated=False\n",
      "step=80 reward=0.14165259163772553 terminated=False\n",
      "step=81 reward=0.16254716720209392 terminated=False\n",
      "step=82 reward=0.18038377158048674 terminated=False\n",
      "step=83 reward=0.1955349503035339 terminated=False\n",
      "step=84 reward=0.209590275273389 terminated=False\n",
      "step=85 reward=0.22297886936917977 terminated=False\n",
      "step=86 reward=0.2392411144470952 terminated=False\n",
      "step=87 reward=0.2580199500579908 terminated=False\n",
      "step=88 reward=0.2777329773794388 terminated=False\n",
      "step=89 reward=0.29761240390900523 terminated=False\n",
      "step=90 reward=0.31716465663195625 terminated=False\n",
      "step=91 reward=0.3355213586040695 terminated=False\n",
      "step=92 reward=0.3524998804211362 terminated=False\n",
      "step=93 reward=0.36810918114197516 terminated=False\n",
      "step=94 reward=0.38014989945536126 terminated=False\n",
      "step=95 reward=0.3905574417126761 terminated=False\n",
      "step=96 reward=0.4002848213657343 terminated=False\n",
      "step=97 reward=0.4170736338625133 terminated=False\n",
      "step=98 reward=0.44908708220841653 terminated=False\n",
      "step=99 reward=0.49331303205450905 terminated=False\n",
      "step=100 reward=0.5554300692787169 terminated=False\n",
      "step=101 reward=0.6250862544059551 terminated=False\n",
      "step=102 reward=0.6893315351179427 terminated=False\n",
      "step=103 reward=0.7439593776734601 terminated=False\n",
      "step=104 reward=0.7813831447770964 terminated=False\n",
      "step=105 reward=0.8090518882538443 terminated=False\n",
      "step=106 reward=0.8280920189882142 terminated=False\n",
      "step=107 reward=0.8437796249119314 terminated=False\n",
      "step=108 reward=0.8639611941043245 terminated=False\n",
      "step=109 reward=0.8906448932017331 terminated=False\n",
      "step=110 reward=0.917804351543538 terminated=False\n",
      "step=111 reward=0.9421491802064923 terminated=False\n",
      "step=112 reward=0.9619641827581312 terminated=False\n",
      "step=113 reward=0.9752323060631967 terminated=False\n",
      "step=114 reward=0.9769516793273336 terminated=False\n",
      "step=115 reward=0.9769516793273336 terminated=False\n",
      "step=116 reward=0.9769516793273336 terminated=False\n",
      "step=117 reward=0.9791212865711658 terminated=False\n",
      "step=118 reward=0.9821981164893177 terminated=False\n",
      "step=119 reward=0.9825801568345679 terminated=False\n",
      "step=120 reward=0.9825801568345679 terminated=False\n",
      "step=121 reward=0.9825801568345679 terminated=False\n",
      "step=122 reward=0.9825801568345679 terminated=False\n",
      "step=123 reward=0.9825801568345679 terminated=False\n",
      "step=124 reward=0.9825801568345679 terminated=False\n",
      "step=125 reward=0.9825801568345679 terminated=False\n",
      "step=126 reward=0.9825801568345679 terminated=False\n",
      "step=127 reward=0.9825801568345679 terminated=False\n",
      "step=128 reward=0.9825801568345679 terminated=False\n",
      "step=129 reward=0.9825801568345679 terminated=False\n",
      "step=130 reward=0.9825801568345679 terminated=False\n",
      "step=131 reward=0.9825801568345679 terminated=False\n",
      "step=132 reward=0.9825801568345679 terminated=False\n",
      "step=133 reward=0.9825801568345679 terminated=False\n",
      "step=134 reward=0.9825801568345679 terminated=False\n",
      "step=135 reward=0.9825801568345679 terminated=False\n",
      "step=136 reward=0.9825801568345679 terminated=False\n",
      "step=137 reward=0.9825801568345679 terminated=False\n",
      "step=138 reward=0.9825801568345679 terminated=False\n",
      "step=139 reward=0.9825801568345679 terminated=False\n",
      "step=140 reward=0.9864498413092376 terminated=False\n",
      "step=141 reward=0.9976040425707386 terminated=False\n",
      "step=142 reward=1.0 terminated=True\n",
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (680, 680) to (688, 688) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "[swscaler @ 0x5a7cec0] Warning: data is not aligned! This can lead to a speed loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video of the evaluation is available in 'outputs/eval/example_pusht_diffusion/rollout.mp4'.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Video' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m imageio\u001b[38;5;241m.\u001b[39mmimsave(\u001b[38;5;28mstr\u001b[39m(video_path), numpy\u001b[38;5;241m.\u001b[39mstack(frames), fps\u001b[38;5;241m=\u001b[39mfps)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo of the evaluation is available in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[43mVideo\u001b[49m(video_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Video' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare to collect every rewards and all the frames of the episode,\n",
    "# from initial state to final state.\n",
    "rewards = []\n",
    "frames = []\n",
    "\n",
    "# Render frame of the initial state\n",
    "frames.append(env.render())\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "while not done:\n",
    "    # Prepare observation for the policy running in Pytorch\n",
    "    state = torch.from_numpy(numpy_observation[\"agent_pos\"])\n",
    "    image = torch.from_numpy(numpy_observation[\"pixels\"])\n",
    "\n",
    "    # Convert to float32 with image from channel first in [0,255]\n",
    "    # to channel last in [0,1]\n",
    "    state = state.to(torch.float32)\n",
    "    image = image.to(torch.float32) / 255\n",
    "    image = image.permute(2, 0, 1)\n",
    "\n",
    "    # Send data tensors from CPU to GPU\n",
    "    state = state.to(device, non_blocking=True)\n",
    "    image = image.to(device, non_blocking=True)\n",
    "\n",
    "    # Add extra (empty) batch dimension, required to forward the policy\n",
    "    state = state.unsqueeze(0)\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # Create the policy input dictionary\n",
    "    observation = {\n",
    "        \"observation.state\": state,\n",
    "        \"observation.image\": image,\n",
    "    }\n",
    "\n",
    "    # Predict the next action with respect to the current observation\n",
    "    with torch.inference_mode():\n",
    "        action = policy.select_action(observation)\n",
    "\n",
    "    # Prepare the action for the environment\n",
    "    numpy_action = action.squeeze(0).to(\"cpu\").numpy()\n",
    "\n",
    "    # Step through the environment and receive a new observation\n",
    "    numpy_observation, reward, terminated, truncated, info = env.step(numpy_action)\n",
    "    print(f\"{step=} {reward=} {terminated=}\")\n",
    "\n",
    "    # Keep track of all the rewards and frames\n",
    "    rewards.append(reward)\n",
    "    frames.append(env.render())\n",
    "\n",
    "    # The rollout is considered done when the success state is reach (i.e. terminated is True),\n",
    "    # or the maximum number of iterations is reached (i.e. truncated is True)\n",
    "    done = terminated | truncated | done\n",
    "    step += 1\n",
    "\n",
    "if terminated:\n",
    "    print(\"Success!\")\n",
    "else:\n",
    "    print(\"Failure!\")\n",
    "\n",
    "# Get the speed of environment (i.e. its number of frames per second).\n",
    "fps = env.metadata[\"render_fps\"]\n",
    "\n",
    "# Encode all frames into a mp4 video.\n",
    "video_path = output_directory / \"rollout.mp4\"\n",
    "imageio.mimsave(str(video_path), numpy.stack(frames), fps=fps)\n",
    "\n",
    "print(f\"Video of the evaluation is available in '{video_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd62065-1ac1-4cff-9fd6-236d490b3808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"outputs/eval/example_pusht_diffusion/rollout.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f687655a-a892-4864-9bb6-8e396d8dd72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 96, 96])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation['observation.image'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot",
   "language": "python",
   "name": "lerobot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
